#[version = "0.0.5"]
def @main(%input_1: Tensor[(1, 28, 28, 1), float32], %v_param_1: Tensor[(3, 3, 1, 32), float32], %v_param_2: Tensor[(32), float32], %v_param_3: Tensor[(3, 3, 32, 64), float32], %v_param_4: Tensor[(64), float32], %v_param_5: Tensor[(2, 256), float32], %v_param_6: Tensor[(2), float32]) {
  %0 = nn.conv2d(%input_1, %v_param_1, strides=[2, 2], padding=[0, 0, 1, 1], channels=32, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO");
  %1 = nn.bias_add(%0, %v_param_2, axis=-1);
  %2 = nn.relu(%1);
  %3 = nn.max_pool2d(%2, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0], layout="NHWC");
  %4 = nn.conv2d(%3, %v_param_3, padding=[0, 0, 0, 0], channels=64, kernel_size=[3, 3], data_layout="NHWC", kernel_layout="HWIO");
  %5 = nn.bias_add(%4, %v_param_4, axis=-1);
  %6 = nn.relu(%5);
  %7 = nn.max_pool2d(%6, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0], layout="NHWC");
  %8 = nn.batch_flatten(%7);
  %9 = nn.dense(%8, %v_param_5, units=2);
  %10 = nn.bias_add(%9, %v_param_6);
  nn.softmax(%10)
}