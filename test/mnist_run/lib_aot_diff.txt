1c1
< // tvm target: c -keys=cpu -link-params=1
---
> // tvm target: c -keys=cpu -link-params=1 -mcpu=core-avx2
2476,2535c2476
< TVM_DLL int32_t fused_nn_softmax(void* args, void* arg_type_ids, int32_t num_args, void* out_ret_value, void* out_ret_tcode, void* resource_handle) {
<   void* arg0 = (((TVMValue*)args)[0].v_handle);
<   int32_t arg0_code = ((int32_t*)arg_type_ids)[(0)];
<   void* arg1 = (((TVMValue*)args)[1].v_handle);
<   int32_t arg1_code = ((int32_t*)arg_type_ids)[(1)];
<   void* placeholder = (((DLTensor*)arg0)[0].data);
<   void* arg0_shape = (((DLTensor*)arg0)[0].shape);
<   void* arg0_strides = (((DLTensor*)arg0)[0].strides);
<   int32_t dev_id = (((DLTensor*)arg0)[0].device.device_id);
<   void* T_softmax_norm = (((DLTensor*)arg1)[0].data);
<   void* arg1_shape = (((DLTensor*)arg1)[0].shape);
<   void* arg1_strides = (((DLTensor*)arg1)[0].strides);
<   if (!(arg0_strides == NULL)) {
<   }
<   if (!(arg1_strides == NULL)) {
<   }
<   void* T_softmax_maxelem = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)4, 2, 32);
<   if (T_softmax_maxelem == NULL) {
<     return -1;
<   }
<   void* T_softmax_exp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)8, 2, 32);
<   if (T_softmax_exp == NULL) {
<     return -1;
<   }
<   void* T_softmax_expsum = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)4, 2, 32);
<   if (T_softmax_expsum == NULL) {
<     return -1;
<   }
<   ((float*)T_softmax_maxelem)[(0)] = -3.402823e+38f;
<   for (int32_t k = 0; k < 2; ++k) {
<     float _1 = ((float*)T_softmax_maxelem)[(0)];
<     float _2 = ((float*)placeholder)[(k)];
<     ((float*)T_softmax_maxelem)[(0)] = ((_1) > (_2) ? (_1) : (_2));
<   }
<   for (int32_t i1 = 0; i1 < 2; ++i1) {
<     ((float*)T_softmax_exp)[(i1)] = expf((((float*)placeholder)[(i1)] - ((float*)T_softmax_maxelem)[(0)]));
<   }
<   ((float*)T_softmax_expsum)[(0)] = 0.000000e+00f;
<   for (int32_t k1 = 0; k1 < 2; ++k1) {
<     ((float*)T_softmax_expsum)[(0)] = (((float*)T_softmax_expsum)[(0)] + ((float*)T_softmax_exp)[(k1)]);
<   }
<   for (int32_t i11 = 0; i11 < 2; ++i11) {
<     ((float*)T_softmax_norm)[(i11)] = (((float*)T_softmax_exp)[(i11)] / ((float*)T_softmax_expsum)[(0)]);
<   }
<   if (TVMBackendFreeWorkspace(1, dev_id, T_softmax_expsum) != 0) {
<     return -1;
<   }
<   if (TVMBackendFreeWorkspace(1, dev_id, T_softmax_exp) != 0) {
<     return -1;
<   }
<   if (TVMBackendFreeWorkspace(1, dev_id, T_softmax_maxelem) != 0) {
<     return -1;
<   }
<   return 0;
< }
< 
< #ifdef __cplusplus
< extern "C"
< #endif
< TVM_DLL int32_t fused_nn_max_pool2d_1(void* args, void* arg_type_ids, int32_t num_args, void* out_ret_value, void* out_ret_tcode, void* resource_handle) {
---
> TVM_DLL int32_t fused_nn_batch_flatten(void* args, void* arg_type_ids, int32_t num_args, void* out_ret_value, void* out_ret_tcode, void* resource_handle) {
2551,2562c2492,2494
<   for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 7; ++ax0_ax1_fused) {
<     for (int32_t ax2 = 0; ax2 < 7; ++ax2) {
<       for (int32_t ax3_init = 0; ax3_init < 32; ++ax3_init) {
<         ((float*)tensor)[((((ax0_ax1_fused * 224) + (ax2 * 32)) + ax3_init))] = -3.402823e+38f;
<       }
<       for (int32_t rv0_rv1_fused = 0; rv0_rv1_fused < 4; ++rv0_rv1_fused) {
<         for (int32_t ax3 = 0; ax3 < 32; ++ax3) {
<           float _1 = ((float*)tensor)[((((ax0_ax1_fused * 224) + (ax2 * 32)) + ax3))];
<           float _2 = ((float*)placeholder)[((((((ax0_ax1_fused * 896) + ((rv0_rv1_fused >> 1) * 448)) + (ax2 * 64)) + ((rv0_rv1_fused & 1) * 32)) + ax3))];
<           ((float*)tensor)[((((ax0_ax1_fused * 224) + (ax2 * 32)) + ax3))] = ((_1) > (_2) ? (_1) : (_2));
<         }
<       }
---
>   for (int32_t ax1_outer = 0; ax1_outer < 16; ++ax1_outer) {
>     for (int32_t ax1_inner = 0; ax1_inner < 16; ++ax1_inner) {
>       ((float*)tensor)[(((ax1_outer * 16) + ax1_inner))] = ((float*)placeholder)[(((ax1_outer * 16) + ax1_inner))];
2571c2503
< TVM_DLL int32_t fused_nn_conv2d_add_nn_relu(void* args, void* arg_type_ids, int32_t num_args, void* out_ret_value, void* out_ret_tcode, void* resource_handle) {
---
> TVM_DLL int32_t fused_nn_conv2d_add_nn_relu_1(void* args, void* arg_type_ids, int32_t num_args, void* out_ret_value, void* out_ret_tcode, void* resource_handle) {
2601c2533
<   void* PaddedInput = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)6272, 2, 32);
---
>   void* PaddedInput = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)3364, 2, 32);
2605,2609c2537,2539
<   for (int32_t i0_i1_fused = 0; i0_i1_fused < 7; ++i0_i1_fused) {
<     for (int32_t i2 = 0; i2 < 7; ++i2) {
<       for (int32_t i3 = 0; i3 < 32; ++i3) {
<         ((float*)PaddedInput)[((((i0_i1_fused * 224) + (i2 * 32)) + i3))] = ((float*)placeholder)[((((i0_i1_fused * 224) + (i2 * 32)) + i3))];
<       }
---
>   for (int32_t i0_i1_fused = 0; i0_i1_fused < 29; ++i0_i1_fused) {
>     for (int32_t i2 = 0; i2 < 29; ++i2) {
>       ((float*)PaddedInput)[(((i0_i1_fused * 29) + i2))] = (((i0_i1_fused < 28) && (i2 < 28)) ? ((float*)placeholder)[(((i0_i1_fused * 28) + i2))] : 0.000000e+00f);
2612,2613c2542,2543
<   for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 25; ++ax0_ax1_fused_ax2_fused) {
<     void* Conv2dOutput = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)256, 2, 32);
---
>   for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 196; ++ax0_ax1_fused_ax2_fused) {
>     void* Conv2dOutput = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)4, 2, 32);
2617,2618c2547,2548
<     for (int32_t ff = 0; ff < 64; ++ff) {
<       ((float*)Conv2dOutput)[(ff)] = 0.000000e+00f;
---
>     for (int32_t ax3 = 0; ax3 < 32; ++ax3) {
>       ((float*)Conv2dOutput)[(0)] = 0.000000e+00f;
2621,2623c2551
<           for (int32_t rc = 0; rc < 32; ++rc) {
<             ((float*)Conv2dOutput)[(ff)] = (((float*)Conv2dOutput)[(ff)] + (((float*)PaddedInput)[(((((((ax0_ax1_fused_ax2_fused / 5) * 224) + (ry * 224)) + (rx * 32)) + ((ax0_ax1_fused_ax2_fused % 5) * 32)) + rc))] * ((float*)placeholder1)[(((((ry * 6144) + (rx * 2048)) + (rc * 64)) + ff))]));
<           }
---
>           ((float*)Conv2dOutput)[(0)] = (((float*)Conv2dOutput)[(0)] + (((float*)PaddedInput)[((((((ax0_ax1_fused_ax2_fused / 14) * 58) + (ry * 29)) + ((ax0_ax1_fused_ax2_fused % 14) * 2)) + rx))] * ((float*)placeholder1)[((((ry * 96) + (rx * 32)) + ax3))]));
2626,2629c2554,2555
<     }
<     for (int32_t ax3_inner = 0; ax3_inner < 64; ++ax3_inner) {
<       float _1 = ((float*)Conv2dOutput)[(ax3_inner)] + ((float*)placeholder2)[(ax3_inner)];
<       ((float*)T_relu)[(((ax0_ax1_fused_ax2_fused * 64) + ax3_inner))] = ((_1) > (0.000000e+00f) ? (_1) : (0.000000e+00f));
---
>       float _1 = ((float*)Conv2dOutput)[(0)] + ((float*)placeholder2)[(ax3)];
>       ((float*)T_relu)[(((ax0_ax1_fused_ax2_fused * 32) + ax3))] = ((_1) > (0.000000e+00f) ? (_1) : (0.000000e+00f));
2644c2570
< TVM_DLL int32_t fused_nn_conv2d_add_nn_relu_1(void* args, void* arg_type_ids, int32_t num_args, void* out_ret_value, void* out_ret_tcode, void* resource_handle) {
---
> TVM_DLL int32_t fused_nn_contrib_dense_pack_add(void* args, void* arg_type_ids, int32_t num_args, void* out_ret_value, void* out_ret_tcode, void* resource_handle) {
2663c2589
<   void* T_relu = (((DLTensor*)arg3)[0].data);
---
>   void* T_add = (((DLTensor*)arg3)[0].data);
2674,2675c2600,2601
<   void* PaddedInput = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)3364, 2, 32);
<   if (PaddedInput == NULL) {
---
>   void* compute_global = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)8, 2, 32);
>   if (compute_global == NULL) {
2678,2681c2604,2605
<   for (int32_t i0_i1_fused = 0; i0_i1_fused < 29; ++i0_i1_fused) {
<     for (int32_t i2 = 0; i2 < 29; ++i2) {
<       ((float*)PaddedInput)[(((i0_i1_fused * 29) + i2))] = (((i0_i1_fused < 28) && (i2 < 28)) ? ((float*)placeholder)[(((i0_i1_fused * 28) + i2))] : 0.000000e+00f);
<     }
---
>   for (int32_t x_c_init = 0; x_c_init < 2; ++x_c_init) {
>     ((float*)compute_global)[(x_c_init)] = 0.000000e+00f;
2683,2699c2607,2609
<   for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 196; ++ax0_ax1_fused_ax2_fused) {
<     void* Conv2dOutput = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)4, 2, 32);
<     if (Conv2dOutput == NULL) {
<       return -1;
<     }
<     for (int32_t ax3 = 0; ax3 < 32; ++ax3) {
<       ((float*)Conv2dOutput)[(0)] = 0.000000e+00f;
<       for (int32_t ry = 0; ry < 3; ++ry) {
<         for (int32_t rx = 0; rx < 3; ++rx) {
<           ((float*)Conv2dOutput)[(0)] = (((float*)Conv2dOutput)[(0)] + (((float*)PaddedInput)[((((((ax0_ax1_fused_ax2_fused / 14) * 58) + (ry * 29)) + ((ax0_ax1_fused_ax2_fused % 14) * 2)) + rx))] * ((float*)placeholder1)[((((ry * 96) + (rx * 32)) + ax3))]));
<         }
<       }
<       float _1 = ((float*)Conv2dOutput)[(0)] + ((float*)placeholder2)[(ax3)];
<       ((float*)T_relu)[(((ax0_ax1_fused_ax2_fused * 32) + ax3))] = ((_1) > (0.000000e+00f) ? (_1) : (0.000000e+00f));
<     }
<     if (TVMBackendFreeWorkspace(1, dev_id, Conv2dOutput) != 0) {
<       return -1;
---
>   for (int32_t k_outer = 0; k_outer < 256; ++k_outer) {
>     for (int32_t x_c = 0; x_c < 2; ++x_c) {
>       ((float*)compute_global)[(x_c)] = (((float*)compute_global)[(x_c)] + (((float*)placeholder)[(k_outer)] * ((float*)placeholder1)[(((k_outer * 2) + x_c))]));
2702c2612,2615
<   if (TVMBackendFreeWorkspace(1, dev_id, PaddedInput) != 0) {
---
>   for (int32_t ax1_inner_inner = 0; ax1_inner_inner < 2; ++ax1_inner_inner) {
>     ((float*)T_add)[(ax1_inner_inner)] = (((float*)compute_global)[(ax1_inner_inner)] + ((float*)placeholder2)[(ax1_inner_inner)]);
>   }
>   if (TVMBackendFreeWorkspace(1, dev_id, compute_global) != 0) {
2747c2660
< TVM_DLL int32_t fused_nn_batch_flatten(void* args, void* arg_type_ids, int32_t num_args, void* out_ret_value, void* out_ret_tcode, void* resource_handle) {
---
> TVM_DLL int32_t fused_nn_max_pool2d_1(void* args, void* arg_type_ids, int32_t num_args, void* out_ret_value, void* out_ret_tcode, void* resource_handle) {
2763,2765c2676,2687
<   for (int32_t ax1_outer = 0; ax1_outer < 16; ++ax1_outer) {
<     for (int32_t ax1_inner = 0; ax1_inner < 16; ++ax1_inner) {
<       ((float*)tensor)[(((ax1_outer * 16) + ax1_inner))] = ((float*)placeholder)[(((ax1_outer * 16) + ax1_inner))];
---
>   for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 7; ++ax0_ax1_fused) {
>     for (int32_t ax2 = 0; ax2 < 7; ++ax2) {
>       for (int32_t ax3_init = 0; ax3_init < 32; ++ax3_init) {
>         ((float*)tensor)[((((ax0_ax1_fused * 224) + (ax2 * 32)) + ax3_init))] = -3.402823e+38f;
>       }
>       for (int32_t rv0_rv1_fused = 0; rv0_rv1_fused < 4; ++rv0_rv1_fused) {
>         for (int32_t ax3 = 0; ax3 < 32; ++ax3) {
>           float _1 = ((float*)tensor)[((((ax0_ax1_fused * 224) + (ax2 * 32)) + ax3))];
>           float _2 = ((float*)placeholder)[((((((ax0_ax1_fused * 896) + ((rv0_rv1_fused >> 1) * 448)) + (ax2 * 64)) + ((rv0_rv1_fused & 1) * 32)) + ax3))];
>           ((float*)tensor)[((((ax0_ax1_fused * 224) + (ax2 * 32)) + ax3))] = ((_1) > (_2) ? (_1) : (_2));
>         }
>       }
2774c2696
< TVM_DLL int32_t fused_nn_contrib_dense_pack_add(void* args, void* arg_type_ids, int32_t num_args, void* out_ret_value, void* out_ret_tcode, void* resource_handle) {
---
> TVM_DLL int32_t fused_nn_conv2d_add_nn_relu(void* args, void* arg_type_ids, int32_t num_args, void* out_ret_value, void* out_ret_tcode, void* resource_handle) {
2793c2715
<   void* T_add = (((DLTensor*)arg3)[0].data);
---
>   void* T_relu = (((DLTensor*)arg3)[0].data);
2804,2805c2726,2727
<   void* compute_global = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)8, 2, 32);
<   if (compute_global == NULL) {
---
>   void* PaddedInput = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)6272, 2, 32);
>   if (PaddedInput == NULL) {
2808,2809c2730,2735
<   for (int32_t x_c_init = 0; x_c_init < 2; ++x_c_init) {
<     ((float*)compute_global)[(x_c_init)] = 0.000000e+00f;
---
>   for (int32_t i0_i1_fused = 0; i0_i1_fused < 7; ++i0_i1_fused) {
>     for (int32_t i2 = 0; i2 < 7; ++i2) {
>       for (int32_t i3 = 0; i3 < 32; ++i3) {
>         ((float*)PaddedInput)[((((i0_i1_fused * 224) + (i2 * 32)) + i3))] = ((float*)placeholder)[((((i0_i1_fused * 224) + (i2 * 32)) + i3))];
>       }
>     }
2811,2813c2737,2757
<   for (int32_t k_outer = 0; k_outer < 256; ++k_outer) {
<     for (int32_t x_c = 0; x_c < 2; ++x_c) {
<       ((float*)compute_global)[(x_c)] = (((float*)compute_global)[(x_c)] + (((float*)placeholder)[(k_outer)] * ((float*)placeholder1)[(((k_outer * 2) + x_c))]));
---
>   for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 25; ++ax0_ax1_fused_ax2_fused) {
>     void* Conv2dOutput = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)256, 2, 32);
>     if (Conv2dOutput == NULL) {
>       return -1;
>     }
>     for (int32_t ff = 0; ff < 64; ++ff) {
>       ((float*)Conv2dOutput)[(ff)] = 0.000000e+00f;
>       for (int32_t ry = 0; ry < 3; ++ry) {
>         for (int32_t rx = 0; rx < 3; ++rx) {
>           for (int32_t rc = 0; rc < 32; ++rc) {
>             ((float*)Conv2dOutput)[(ff)] = (((float*)Conv2dOutput)[(ff)] + (((float*)PaddedInput)[(((((((ax0_ax1_fused_ax2_fused / 5) * 224) + (ry * 224)) + (rx * 32)) + ((ax0_ax1_fused_ax2_fused % 5) * 32)) + rc))] * ((float*)placeholder1)[(((((ry * 6144) + (rx * 2048)) + (rc * 64)) + ff))]));
>           }
>         }
>       }
>     }
>     for (int32_t ax3_inner = 0; ax3_inner < 64; ++ax3_inner) {
>       float _1 = ((float*)Conv2dOutput)[(ax3_inner)] + ((float*)placeholder2)[(ax3_inner)];
>       ((float*)T_relu)[(((ax0_ax1_fused_ax2_fused * 64) + ax3_inner))] = ((_1) > (0.000000e+00f) ? (_1) : (0.000000e+00f));
>     }
>     if (TVMBackendFreeWorkspace(1, dev_id, Conv2dOutput) != 0) {
>       return -1;
2816,2817c2760,2761
<   for (int32_t ax1_inner_inner = 0; ax1_inner_inner < 2; ++ax1_inner_inner) {
<     ((float*)T_add)[(ax1_inner_inner)] = (((float*)compute_global)[(ax1_inner_inner)] + ((float*)placeholder2)[(ax1_inner_inner)]);
---
>   if (TVMBackendFreeWorkspace(1, dev_id, PaddedInput) != 0) {
>     return -1;
2819c2763,2819
<   if (TVMBackendFreeWorkspace(1, dev_id, compute_global) != 0) {
---
>   return 0;
> }
> 
> #ifdef __cplusplus
> extern "C"
> #endif
> TVM_DLL int32_t fused_nn_softmax(void* args, void* arg_type_ids, int32_t num_args, void* out_ret_value, void* out_ret_tcode, void* resource_handle) {
>   void* arg0 = (((TVMValue*)args)[0].v_handle);
>   int32_t arg0_code = ((int32_t*)arg_type_ids)[(0)];
>   void* arg1 = (((TVMValue*)args)[1].v_handle);
>   int32_t arg1_code = ((int32_t*)arg_type_ids)[(1)];
>   void* placeholder = (((DLTensor*)arg0)[0].data);
>   void* arg0_shape = (((DLTensor*)arg0)[0].shape);
>   void* arg0_strides = (((DLTensor*)arg0)[0].strides);
>   int32_t dev_id = (((DLTensor*)arg0)[0].device.device_id);
>   void* T_softmax_norm = (((DLTensor*)arg1)[0].data);
>   void* arg1_shape = (((DLTensor*)arg1)[0].shape);
>   void* arg1_strides = (((DLTensor*)arg1)[0].strides);
>   if (!(arg0_strides == NULL)) {
>   }
>   if (!(arg1_strides == NULL)) {
>   }
>   void* T_softmax_maxelem = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)4, 2, 32);
>   if (T_softmax_maxelem == NULL) {
>     return -1;
>   }
>   void* T_softmax_exp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)8, 2, 32);
>   if (T_softmax_exp == NULL) {
>     return -1;
>   }
>   void* T_softmax_expsum = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)4, 2, 32);
>   if (T_softmax_expsum == NULL) {
>     return -1;
>   }
>   ((float*)T_softmax_maxelem)[(0)] = -3.402823e+38f;
>   for (int32_t k = 0; k < 2; ++k) {
>     float _1 = ((float*)T_softmax_maxelem)[(0)];
>     float _2 = ((float*)placeholder)[(k)];
>     ((float*)T_softmax_maxelem)[(0)] = ((_1) > (_2) ? (_1) : (_2));
>   }
>   for (int32_t i1 = 0; i1 < 2; ++i1) {
>     ((float*)T_softmax_exp)[(i1)] = expf((((float*)placeholder)[(i1)] - ((float*)T_softmax_maxelem)[(0)]));
>   }
>   ((float*)T_softmax_expsum)[(0)] = 0.000000e+00f;
>   for (int32_t k1 = 0; k1 < 2; ++k1) {
>     ((float*)T_softmax_expsum)[(0)] = (((float*)T_softmax_expsum)[(0)] + ((float*)T_softmax_exp)[(k1)]);
>   }
>   for (int32_t i11 = 0; i11 < 2; ++i11) {
>     ((float*)T_softmax_norm)[(i11)] = (((float*)T_softmax_exp)[(i11)] / ((float*)T_softmax_expsum)[(0)]);
>   }
>   if (TVMBackendFreeWorkspace(1, dev_id, T_softmax_expsum) != 0) {
>     return -1;
>   }
>   if (TVMBackendFreeWorkspace(1, dev_id, T_softmax_exp) != 0) {
>     return -1;
>   }
>   if (TVMBackendFreeWorkspace(1, dev_id, T_softmax_maxelem) != 0) {
